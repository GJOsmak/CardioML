{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn import metrics, linear_model\n",
    "from random import choices\n",
    "from itertools import compress\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "\n",
    "class Data(object): \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "Data.X = Data()\n",
    "Data.X.train = pd.read_csv('./Data/Sets/train_GSE36961.csv', index_col=0)\n",
    "Data.X.test = pd.read_csv('./Data/Sets/test_rna_seq_data.csv', index_col=0)\n",
    "Data.y = Data()\n",
    "Data.y.train = list(pd.read_csv('./Data/Sets/train_GSE36961_target.csv').iloc[:,1])\n",
    "Data.y.test = list(pd.read_csv('./Data/Sets/test_rna_seq_target.csv').iloc[:,1])\n",
    "\n",
    "class FeatureExtraction(object):\n",
    "    \"\"\"\n",
    "    Класс для экстракции фичей. Главная идея не фиксировать случайность, а оседлать её :)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit_inner_loop(self, X_train, X_test, y_train, y_test, C=0.03):\n",
    "\n",
    "        \"\"\"\n",
    "        Будем n_iter раз бутстрепить сбалансированную train выборку из X_train.\n",
    "        Обучаем лог.рег. с L1-решуляризацией, с коэффициентом как мы отобрали выше.\n",
    "        Тестим на X_test, значение добавляем в roc_auc_list\n",
    "\n",
    "        Если на X_test модель работает круче 0.7, то: \n",
    "            1) ненулевые фичи модели добавляем в словарик отобранных фичей feature_dict\n",
    "            2) обновляем число фичей в перменной len_best_feature = len(feature_dict.keys())\n",
    "        Если нет, то:\n",
    "            3) дублируем последнее значение в len_best_feature, т.к. число фичей не изменилось\n",
    "        \"\"\"\n",
    "\n",
    "        len_best_feature = [0]  # заводим лист, в котором будем отслеживать изменение количества фичей\n",
    "        len_best_more_one = [\n",
    "            0]  # заводим лист, в котором будем отслеживать изменение количества числа включений уже включенных фичей\n",
    "        roc_auc_list = list()  # аналогично, отслеживаем как меняется roc-auc, так для интереса\n",
    "        feature_dict = dict()  # # словарь \"ген: log.reg.coef\"\n",
    "\n",
    "        # чтобы получить сбаланнсированную выборку, бутстрепим отдельно семплы из контроля и из опыта\n",
    "\n",
    "        mask = np.array(y_train) == 0\n",
    "\n",
    "        k_len = min(len(mask) - sum(mask), sum(mask))  # размер выборки бутстрепа, берем размер минимальной группы HCM или CTRL\n",
    "\n",
    "        CTRL_rows = list(compress(range(0, len(mask)), mask))\n",
    "        HCM_rows = list(compress(range(0, len(mask)), mask == False))\n",
    "\n",
    "        _HCM_rows = choices(HCM_rows, k=k_len)  # бутстрепим номера строк из группы больных\n",
    "        _CTRL_rows = choices(CTRL_rows, k=k_len)  # бутстрепим номера строк из группы здоровых\n",
    "\n",
    "        # объединяем это всё дело обратно\n",
    "\n",
    "        _X_train = pd.DataFrame(X_train).iloc[_HCM_rows + _CTRL_rows, :]\n",
    "        _y_train = np.array(y_train)[_HCM_rows + _CTRL_rows]\n",
    "        print(_X_train.index)\n",
    "\n",
    "        # обучаем лог.рег. с ранее отобранным коэффициентом регуляризации\n",
    "\n",
    "        linear_regressor = linear_model.LogisticRegression(penalty='l1', C=C, solver='liblinear',\n",
    "                                                           random_state=42)\n",
    "        linear_regressor.fit(_X_train, _y_train)\n",
    "\n",
    "        # тестим\n",
    "\n",
    "        roc_auc = metrics.roc_auc_score(y_score=linear_regressor.predict(X_test), y_true=y_test)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "\n",
    "        # далее отбираем фичи из моделей, которые хоть как-то работают (roc_auc > 0.7)\n",
    "\n",
    "        if roc_auc > 0.7:\n",
    "            # отбираем смысловые фичи\n",
    "            mask = linear_regressor.coef_ != 0\n",
    "            genes = X_train.columns[mask[0]]\n",
    "            values = linear_regressor.coef_[mask]\n",
    "\n",
    "            _feature_dict = dict(zip(genes, abs(values) * roc_auc))  # делаем временный словарь \"ген: его ценность\"\n",
    "\n",
    "            # обнавляем глобальный словарь фичей\n",
    "            for gene, values in _feature_dict.items():\n",
    "                if gene in feature_dict:\n",
    "                    feature_dict[gene].append(values)\n",
    "                else:\n",
    "                    feature_dict[gene] = [values]\n",
    "\n",
    "            len_best_feature.append(len(feature_dict.keys()))\n",
    "\n",
    "            feature_distr = np.array(list(map(lambda x: len(feature_dict[x]), feature_dict)))\n",
    "            len_best_more_one.append(sum(feature_distr[feature_distr > 1]))\n",
    "\n",
    "        else:\n",
    "            # если модель была говёной, то просто дублируем предыдущее значение. Ну, число фичей то не изменилось :)\n",
    "            len_best_feature.append(len_best_feature[-1])\n",
    "            len_best_more_one.append(len_best_more_one[-1])\n",
    "\n",
    "        self.len_best_more_one = np.array(len_best_more_one)\n",
    "        self.len_best_feature = np.array(len_best_feature)\n",
    "        self.feature_dict = feature_dict\n",
    "        self.roc_auc_list = roc_auc_list\n",
    "\n",
    "    def fit_all_loops(self, X, y, inner_itter, extr_itter):\n",
    "\n",
    "        \"\"\"\n",
    "        Идея: повторим экстракцию фичей n раз,\n",
    "        отсортируем каждый из получившихся наборов по важности фичей, найдем размер окна для отбора n-топ фичей,\n",
    "        в котором состав фичей минимально изменяется от набора к набору. А потом отберем те фичи, которые всегда встречаются в окне этого размера.\n",
    "        \"\"\"\n",
    "\n",
    "        list_feature_dicts = list()\n",
    "\n",
    "        for i in range(0, extr_itter):\n",
    "            print(i, 'external loop fitting...', sep=' ')\n",
    "            # train_test split and transformation\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=i)\n",
    "\n",
    "            normalizer = Normalizer()\n",
    "\n",
    "            X_train = pd.DataFrame(normalizer.fit_transform(X_train), columns=X_train.columns)\n",
    "            X_test = pd.DataFrame(normalizer.fit_transform(X_test), columns=X_test.columns)\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "            X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "            # отбор фичей\n",
    "            fe = FeatureExtraction()\n",
    "            fe.fit_inner_loop(inner_itter,\n",
    "                              X_train, X_test, y_train, y_test)\n",
    "            list_feature_dicts.append(fe.feature_dict)\n",
    "\n",
    "        self.list_feature_dicts = list_feature_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 / 10\n",
      "Iteration: 1 / 10\n",
      "Iteration: 2 / 10\n",
      "Iteration: 3 / 10\n",
      "Iteration: 4 / 10\n",
      "Iteration: 5 / 10\n",
      "Iteration: 6 / 10\n",
      "Iteration: 7 / 10\n",
      "Iteration: 8 / 10\n",
      "Iteration: 9 / 10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn import metrics, linear_model\n",
    "from random import choices\n",
    "from itertools import compress\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from multiprocessing import Manager, Pool, cpu_count\n",
    "\n",
    "NUM_ITERATIONS = 1000\n",
    "NUM_PER_FILE = 100\n",
    "NUM_PROCESSES = 4\n",
    "\n",
    "class Data(object): \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "Data.X = Data()\n",
    "Data.X.train = pd.read_csv('./Data/Sets/train_GSE36961.csv', index_col=0)\n",
    "Data.y = Data()\n",
    "Data.y.train = list(pd.read_csv('./Data/Sets/train_GSE36961_target.csv').iloc[:,1])\n",
    "\n",
    "X_shared = pd.DataFrame(Data.X.train.transpose())\n",
    "y_shared = Data.y.train\n",
    "\n",
    "def MC_model(X, y, C=0.03):\n",
    "#        X = X_shared, \n",
    "#        y = y_shared\n",
    "\n",
    "        mask = np.array(y) == 0\n",
    "\n",
    "        # размер выборки бутстрепа, берем размер минимальной группы HCM или CTRL\n",
    "        k_len = min(len(mask) - sum(mask), sum(mask))  \n",
    "\n",
    "        CTRL_idx = list(compress(range(0, len(mask)), mask))\n",
    "        HCM_idx = list(compress(range(0, len(mask)), mask == False))\n",
    "\n",
    "        train_idx = choices(HCM_idx, k=k_len) + choices(CTRL_idx, k=k_len) # бутстрепим номера строк из группы больных и здоровых\n",
    "        test_idx = list(set([i for i in range(len(mask))]) - set(train_idx))\n",
    "\n",
    "        # объединяем это всё дело обратно\n",
    "\n",
    "        _X_train = pd.DataFrame(X).iloc[train_idx, :]\n",
    "        _y_train = np.array(y)[train_idx]\n",
    "\n",
    "        # обучаем лог.рег. с ранее отобранным коэффициентом регуляризации\n",
    "\n",
    "        linear_regressor = linear_model.LogisticRegression(penalty='l1', C=C, solver='liblinear',\n",
    "                                                           random_state=42)\n",
    "        linear_regressor.fit(_X_train, _y_train)\n",
    "\n",
    "        # тестим\n",
    "\n",
    "        roc_auc = metrics.roc_auc_score(y_score=linear_regressor.predict(pd.DataFrame(X).iloc[test_idx, :]), y_true=np.array(y)[test_idx])\n",
    "\n",
    "        # далее отбираем фичи из моделей, которые хоть как-то работают (roc_auc > 0.7)\n",
    "\n",
    "        if roc_auc > 0.7:\n",
    "            # отбираем смысловые фичи\n",
    "            mask = linear_regressor.coef_ != 0\n",
    "            mask = np.append(mask[0], roc_auc)\n",
    "        \n",
    "        return mask\n",
    "\n",
    "def run_iteration(seed):\n",
    "    np.random.seed(seed)\n",
    "    val = MC_model(X_shared, y_shared)\n",
    "    return val\n",
    "\n",
    "# mgr = Manager()\n",
    "# ns = mgr.Namespace()\n",
    "# ns.X = X_shared\n",
    "# ns.y = y_shared\n",
    "\n",
    "for i in range(NUM_ITERATIONS // NUM_PER_FILE):\n",
    "    print(\"Iteration:\", i, \"/\", NUM_ITERATIONS // NUM_PER_FILE)\n",
    "    \n",
    "    with Pool(NUM_PROCESSES) as p:\n",
    "        res = p.map(run_iteration, [seed for seed in range(i*NUM_PER_FILE, (i+1)*NUM_PER_FILE)])\n",
    "        \n",
    "    out_df = pd.DataFrame.from_records(res)\n",
    "    if i == 0:\n",
    "        # create the initial file\n",
    "        # write the data in a form of pandas data frame\n",
    "        out_df.to_csv('./MC_res1.csv', header=False, index = False)\n",
    "    else:\n",
    "        # append it to the file\n",
    "        out_df.to_csv('./MC_res1.csv', mode='a', header=False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Manager, Pool, cpu_count\n",
    "\n",
    "# def run_iteration(seed):\n",
    "#     np.random.seed(42)\n",
    "#     val = MC_model(ns.X, ns.y)\n",
    "#     return val\n",
    "\n",
    "# mgr = Manager()\n",
    "# ns = mgr.Namespace()\n",
    "# ns.X = X_shared\n",
    "# ns.y = y_shared\n",
    "\n",
    "# with Pool(8) as p:\n",
    "#     res = p.map(run_iteration, [seed for seed in range(0, 200000)])\n",
    "\n",
    "# res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
