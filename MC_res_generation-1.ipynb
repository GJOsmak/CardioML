{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn import metrics, linear_model\n",
    "from random import choices\n",
    "from itertools import compress\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "\n",
    "class Data(object): \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "Data.X = Data()\n",
    "Data.X.train = pd.read_csv('./Data/Sets/train_GSE36961.csv', index_col=0)\n",
    "Data.X.test = pd.read_csv('./Data/Sets/test_rna_seq_data.csv', index_col=0)\n",
    "Data.y = Data()\n",
    "Data.y.train = list(pd.read_csv('./Data/Sets/train_GSE36961_target.csv').iloc[:,1])\n",
    "Data.y.test = list(pd.read_csv('./Data/Sets/test_rna_seq_target.csv').iloc[:,1])\n",
    "\n",
    "class FeatureExtraction(object):\n",
    "    \"\"\"\n",
    "    Класс для экстракции фичей. Главная идея не фиксировать случайность, а оседлать её :)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit_inner_loop(self, X_train, X_test, y_train, y_test, C=0.03):\n",
    "\n",
    "        \"\"\"\n",
    "        Будем n_iter раз бутстрепить сбалансированную train выборку из X_train.\n",
    "        Обучаем лог.рег. с L1-решуляризацией, с коэффициентом как мы отобрали выше.\n",
    "        Тестим на X_test, значение добавляем в roc_auc_list\n",
    "\n",
    "        Если на X_test модель работает круче 0.7, то: \n",
    "            1) ненулевые фичи модели добавляем в словарик отобранных фичей feature_dict\n",
    "            2) обновляем число фичей в перменной len_best_feature = len(feature_dict.keys())\n",
    "        Если нет, то:\n",
    "            3) дублируем последнее значение в len_best_feature, т.к. число фичей не изменилось\n",
    "        \"\"\"\n",
    "\n",
    "        len_best_feature = [0]  # заводим лист, в котором будем отслеживать изменение количества фичей\n",
    "        len_best_more_one = [\n",
    "            0]  # заводим лист, в котором будем отслеживать изменение количества числа включений уже включенных фичей\n",
    "        roc_auc_list = list()  # аналогично, отслеживаем как меняется roc-auc, так для интереса\n",
    "        feature_dict = dict()  # # словарь \"ген: log.reg.coef\"\n",
    "\n",
    "        # чтобы получить сбаланнсированную выборку, бутстрепим отдельно семплы из контроля и из опыта\n",
    "\n",
    "        mask = np.array(y_train) == 0\n",
    "\n",
    "        k_len = min(len(mask) - sum(mask), sum(mask))  # размер выборки бутстрепа, берем размер минимальной группы HCM или CTRL\n",
    "\n",
    "        CTRL_rows = list(compress(range(0, len(mask)), mask))\n",
    "        HCM_rows = list(compress(range(0, len(mask)), mask == False))\n",
    "\n",
    "        _HCM_rows = choices(HCM_rows, k=k_len)  # бутстрепим номера строк из группы больных\n",
    "        _CTRL_rows = choices(CTRL_rows, k=k_len)  # бутстрепим номера строк из группы здоровых\n",
    "\n",
    "        # объединяем это всё дело обратно\n",
    "\n",
    "        _X_train = pd.DataFrame(X_train).iloc[_HCM_rows + _CTRL_rows, :]\n",
    "        _y_train = np.array(y_train)[_HCM_rows + _CTRL_rows]\n",
    "        print(_X_train.index)\n",
    "\n",
    "        # обучаем лог.рег. с ранее отобранным коэффициентом регуляризации\n",
    "\n",
    "        linear_regressor = linear_model.LogisticRegression(penalty='l1', C=C, solver='liblinear',\n",
    "                                                           random_state=42)\n",
    "        linear_regressor.fit(_X_train, _y_train)\n",
    "\n",
    "        # тестим\n",
    "\n",
    "        roc_auc = metrics.roc_auc_score(y_score=linear_regressor.predict(X_test), y_true=y_test)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "\n",
    "        # далее отбираем фичи из моделей, которые хоть как-то работают (roc_auc > 0.7)\n",
    "\n",
    "        if roc_auc > 0.7:\n",
    "            # отбираем смысловые фичи\n",
    "            mask = linear_regressor.coef_ != 0\n",
    "            genes = X_train.columns[mask[0]]\n",
    "            values = linear_regressor.coef_[mask]\n",
    "\n",
    "            _feature_dict = dict(zip(genes, abs(values) * roc_auc))  # делаем временный словарь \"ген: его ценность\"\n",
    "\n",
    "            # обнавляем глобальный словарь фичей\n",
    "            for gene, values in _feature_dict.items():\n",
    "                if gene in feature_dict:\n",
    "                    feature_dict[gene].append(values)\n",
    "                else:\n",
    "                    feature_dict[gene] = [values]\n",
    "\n",
    "            len_best_feature.append(len(feature_dict.keys()))\n",
    "\n",
    "            feature_distr = np.array(list(map(lambda x: len(feature_dict[x]), feature_dict)))\n",
    "            len_best_more_one.append(sum(feature_distr[feature_distr > 1]))\n",
    "\n",
    "        else:\n",
    "            # если модель была говёной, то просто дублируем предыдущее значение. Ну, число фичей то не изменилось :)\n",
    "            len_best_feature.append(len_best_feature[-1])\n",
    "            len_best_more_one.append(len_best_more_one[-1])\n",
    "\n",
    "        self.len_best_more_one = np.array(len_best_more_one)\n",
    "        self.len_best_feature = np.array(len_best_feature)\n",
    "        self.feature_dict = feature_dict\n",
    "        self.roc_auc_list = roc_auc_list\n",
    "\n",
    "    def fit_all_loops(self, X, y, inner_itter, extr_itter):\n",
    "\n",
    "        \"\"\"\n",
    "        Идея: повторим экстракцию фичей n раз,\n",
    "        отсортируем каждый из получившихся наборов по важности фичей, найдем размер окна для отбора n-топ фичей,\n",
    "        в котором состав фичей минимально изменяется от набора к набору. А потом отберем те фичи, которые всегда встречаются в окне этого размера.\n",
    "        \"\"\"\n",
    "\n",
    "        list_feature_dicts = list()\n",
    "\n",
    "        for i in range(0, extr_itter):\n",
    "            print(i, 'external loop fitting...', sep=' ')\n",
    "            # train_test split and transformation\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=i)\n",
    "\n",
    "            normalizer = Normalizer()\n",
    "\n",
    "            X_train = pd.DataFrame(normalizer.fit_transform(X_train), columns=X_train.columns)\n",
    "            X_test = pd.DataFrame(normalizer.fit_transform(X_test), columns=X_test.columns)\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "            X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "            # отбор фичей\n",
    "            fe = FeatureExtraction()\n",
    "            fe.fit_inner_loop(inner_itter,\n",
    "                              X_train, X_test, y_train, y_test)\n",
    "            list_feature_dicts.append(fe.feature_dict)\n",
    "\n",
    "        self.list_feature_dicts = list_feature_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 / 4\n",
      "Iteration: 1 / 4\n",
      "Iteration: 2 / 4\n",
      "Iteration: 3 / 4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn import metrics, linear_model\n",
    "from random import choices\n",
    "from itertools import compress\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from multiprocessing import Manager, Pool, cpu_count\n",
    "\n",
    "NUM_ITERATIONS = 200\n",
    "NUM_PER_FILE = 50\n",
    "NUM_PROCESSES = 4\n",
    "\n",
    "class Data(object): \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "Data.X = Data()\n",
    "Data.X.train = pd.read_csv('./Data/Sets/train_GSE36961.csv', index_col=0)\n",
    "Data.y = Data()\n",
    "Data.y.train = list(pd.read_csv('./Data/Sets/train_GSE36961_target.csv').iloc[:,1])\n",
    "\n",
    "X_shared = pd.DataFrame(Data.X.train.transpose())\n",
    "y_shared = Data.y.train\n",
    "\n",
    "def MC_model(X, y, C=0.03):\n",
    "#        X = X_shared, \n",
    "#        y = y_shared\n",
    "\n",
    "        mask = np.array(y) == 0\n",
    "\n",
    "        # размер выборки бутстрепа, берем размер минимальной группы HCM или CTRL\n",
    "        k_len = min(len(mask) - sum(mask), sum(mask))  \n",
    "\n",
    "        CTRL_idx = list(compress(range(0, len(mask)), mask))\n",
    "        HCM_idx = list(compress(range(0, len(mask)), mask == False))\n",
    "\n",
    "        train_idx = choices(HCM_idx, k=k_len) + choices(CTRL_idx, k=k_len) # бутстрепим номера строк из группы больных и здоровых\n",
    "        test_idx = list(set([i for i in range(len(mask))]) - set(train_idx))\n",
    "\n",
    "        # объединяем это всё дело обратно\n",
    "\n",
    "        _X_train = pd.DataFrame(X).iloc[train_idx, :]\n",
    "        _y_train = np.array(y)[train_idx]\n",
    "        _X_test = pd.DataFrame(X).iloc[test_idx, :]\n",
    "        _y_test = np.array(y)[test_idx]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        _X_train = scaler.fit_transform(_X_train)\n",
    "        _X_test = scaler.transform(_X_test)\n",
    "\n",
    "        # обучаем лог.рег. с ранее отобранным коэффициентом регуляризации\n",
    "\n",
    "        linear_regressor = linear_model.LogisticRegression(penalty='l1', C=C, solver='liblinear',\n",
    "                                                           random_state=42)\n",
    "        linear_regressor.fit(_X_train, _y_train)\n",
    "\n",
    "        # тестим\n",
    "\n",
    "        roc_auc = metrics.roc_auc_score(y_score=linear_regressor.predict(_X_test), y_true=_y_test)\n",
    "\n",
    "        # далее отбираем фичи из моделей, которые хоть как-то работают (roc_auc > 0.7)\n",
    "\n",
    "        if roc_auc > 0.7:\n",
    "            # отбираем смысловые фичи\n",
    "            mask = linear_regressor.coef_ != 0\n",
    "            mask = np.append(mask[0], roc_auc)\n",
    "        \n",
    "        return mask\n",
    "\n",
    "def run_iteration(config):\n",
    "#     val = MC_model(X_shared, y_shared)\n",
    "    val = MC_model(config[0], config[1])\n",
    "    return val\n",
    "\n",
    "# mgr = Manager()\n",
    "# ns = mgr.Namespace()\n",
    "# ns.X = X_shared\n",
    "# ns.y = y_shared\n",
    "\n",
    "np.random.seed(43)\n",
    "\n",
    "for i in range(NUM_ITERATIONS // NUM_PER_FILE):\n",
    "    print(\"Iteration:\", i, \"/\", NUM_ITERATIONS // NUM_PER_FILE)\n",
    "    \n",
    "    with Pool(NUM_PROCESSES) as p:\n",
    "        res = p.map(run_iteration, [(X_shared, y_shared) for _ in range(i*NUM_PER_FILE, (i+1)*NUM_PER_FILE)])\n",
    "        \n",
    "    out_df = pd.DataFrame.from_records(res)\n",
    "    if i == 0:\n",
    "        # create the initial file\n",
    "        # write the data in a form of pandas data frame\n",
    "        out_df.to_csv('./MC_res2.csv', header=False, index = False)\n",
    "    else:\n",
    "        # append it to the file\n",
    "        out_df.to_csv('./MC_res2.csv', mode='a', header=False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "mc_res = pd.read_csv('./MC_res2.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-197c7038e30a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmc_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmc_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, **kwargs)\u001b[0m\n\u001b[1;32m   5689\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5690\u001b[0m             new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors,\n\u001b[0;32m-> 5691\u001b[0;31m                                          **kwargs)\n\u001b[0m\u001b[1;32m   5692\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'astype'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m                                             copy=align_copy)\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, values, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'raise'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         return self._astype(dtype, copy=copy, errors=errors, values=values,\n\u001b[0;32m--> 534\u001b[0;31m                             **kwargs)\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m     def _astype(self, dtype, copy=False, errors='raise', values=None,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m_astype\u001b[0;34m(self, dtype, copy, errors, values, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                     \u001b[0;31m# _astype_nansafe works fine with 1-d only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m                     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0;31m# TODO(extension)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0;31m# Explicit copy, or required since NumPy can't view from / to object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'False'"
     ]
    }
   ],
   "source": [
    "mc_res.loc[:,mc_res.dtypes == 'O'][0].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37702"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mc_res.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(np.matrix(mc_res)).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C    NaN\n",
       "D    2.5\n",
       "dtype: float64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[np.nan, 2, np.nan, 0],\n",
    "                   [3, 4, np.nan, 1],\n",
    "                   ['np.nan', 'np.nan', np.nan, 5],\n",
    "                   [np.nan, 3, np.nan, 4]],\n",
    "                  columns=list(\"ABCD\")).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_res = mc_res.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37702"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mc_res.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 37846 but corresponding boolean dimension is 37701",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1c5e5bab8071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3968\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3969\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3970\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3971\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpromote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 37846 but corresponding boolean dimension is 37701"
     ]
    }
   ],
   "source": [
    "Data.X.train.transpose().columns[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_res1 = pd.read_csv('./MC_res.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37847"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mc_res.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_data = mc_res.append(mc_res1, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19998"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37849"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mc_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (np.array(mc_res1.mean(axis=0)[:-1]) + np.array(mc_res.mean(axis=0)[:-1]) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ACE2', 'APOA1', 'ATP1A2', 'C21ORF7', 'CA3', 'CENPA', 'CLIC6', 'EIF1AY',\n",
       "       'FRZB', 'HS.576694', 'HSPA2', 'IER3', 'LOC100008589', 'MLLT11', 'MXRA5',\n",
       "       'NPPA', 'NPPB', 'PROS1', 'RASD1', 'RASL11B', 'S100A9', 'SERPINA3',\n",
       "       'SFRP1', 'SMOC2', 'THBS4', 'TPM3'],\n",
       "      dtype='object', name='ID_REF')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.X.train.transpose().columns[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Manager, Pool, cpu_count\n",
    "\n",
    "# def run_iteration(seed):\n",
    "#     np.random.seed(42)\n",
    "#     val = MC_model(ns.X, ns.y)\n",
    "#     return val\n",
    "\n",
    "# mgr = Manager()\n",
    "# ns = mgr.Namespace()\n",
    "# ns.X = X_shared\n",
    "# ns.y = y_shared\n",
    "\n",
    "# with Pool(8) as p:\n",
    "#     res = p.map(run_iteration, [seed for seed in range(0, 200000)])\n",
    "\n",
    "# res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
